{
  "title": "Linear Classification",
  "cells": [
    {
      "type": "markdown",
      "data": "* score function: map the raw data to class scores\n* loss function: quantify the agreement between the predicted scores and the ground truth labels\n* Linear Classifier: f(x_{i}, W, b)=Wx_{i}+b\n  * matrix W: K * D, b: K * 1\n![Screen Shot 2018-03-03 at 10.29.22 AM.png](quiver-image-url/E512C6EBB7241C895B6F686C7AE8912A.png =825x677)\n#### Multiclass Support Vector Machine loss\n* The SVM loss is set up so that SVM wants the correct class for each image to have a score higher than the incorrect classes by some fixed margin Î”\nE.g.the score for the j-th class is the j-th element $s_{j} =f(x_{i}, W)_{j}$, the Multiclass SVM loss for the i-th examle is then formalized as: $L_{i}=\\sum\\limits_{j \\neq y_{i}}max(0,s_{j}-s_{y_{i}}+\\bigtriangleup)$\n#### Regularization: encode some preference for a certain set of weights W over others to remove this ambiguity by extending the loss function with a regularization penalty R(W)\n* L2 norm: discourage large weights through an elemenwise quadratic penalty over all parameters\n  * $ R(W)=\\sum\\limits_{k}\\sum\\limits_{l}W^{2}_{k,l} $\n  * summing up all the squared eleents of W\n  * $ L = {\\frac{1}{N}} \\sum\\limits_{i}L_{i}+\\lambda R(W)$\n"
    },
    {
      "type": "code",
      "language": "python",
      "data": "# unvectorized version\ndef L_i (x, y, W):\n  \"\"\"\n  - x is a column vector representing an image \n  - y is an integer giving index of correct class\n  - W is the weight matrix\n  \"\"\"\n  delta = 1.0\n  scores=W.dot(x)\n  correct_class_score=scores[y]\n  D=W.shape[0]\n  loss_i=0.0\n  for j in xrange(D):\n    if(j==y):\n      continue\n    loss_i+=max(0,scores[j-correct_class_score+delta)\n  return loss_i\n# half-vectorized version\ndef L_i_vectorized (x, y, W):\n  delta=1.0\n  score=W.dot(x)\n  margins=np.maximum(0,scores-scores[y]+delta)\n  margins[y]=0\n  loss_i=np.sum(margins)\n  return loss_i\n# fully-vectorized version\ndef L (X, y, W):\n  \"\"\"\n  - X: all the trainign examples (num_of_training*pixels)\n  - y is array of integers specifying correct class\n  - W are weights\n  \"\"\"\n  loss=0.0\n  scores=X.dot(W)\n  #correct_class_score: num_of_train * \n  #correct_class_score[:,np.newaxis]: num_of_train * 1\n  correct_class_score=scores[np.arange(X.shape[0]),y]\n  margins=np.maximum(0,scores-correct_class_score[:,np.newaxis]+1.0)\n  margins[np.arange(X.shape[0]),y]=0\n  loss=np.sum(margins)\n  return loss"
    },
    {
      "type": "markdown",
      "data": "#### Softmax classifier\n* Th function mapping $f(x_{i};W)=Wx_{i}$ stays unchaned, but now interpret these score as the unnormalized log probabilities for each class and replace the hinge loss with a cross-entrpy loss $L_{i}=-log(\\frac{e^{f_{y_{i}}}}{\\sum\\limits_{j}e^{f_{y_{i}}}})$ or $L_{i}=-f_{y_{i}}+log\\sum\\limits_{j}e^{f_{j}}$\n*$f_{j}$ means the j-th element of the vector class scores f*\n* Numeric stability: the intermediate terms $e^{f_{y_{i}}}$ and $\\sum\\limits_{j}e^{f_{y_{i}}}$ might be very large due to the expoentials\n  * multiply the top and bottom of the fraction by a constant C ($C=-max_{j}f_{j}$)"
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}