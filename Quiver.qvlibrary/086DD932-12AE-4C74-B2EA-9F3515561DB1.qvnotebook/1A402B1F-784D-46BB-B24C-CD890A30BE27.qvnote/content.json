{
  "title": "High Level API",
  "cells": [
    {
      "type": "markdown",
      "data": "* Estimator: training + evaluation + prediction + export for serving\n  * Pre-made Estimator: create and manage Graph and Session\n    * write one or more dataset importing functions\n> one function to import the training set, another function to import the test set, return a dictionary in which the keys are feature names and the bales are Tensors containing the corresponding feature data and a Tensor containing one or more labels\n    * define the feature columns: identify a feature name, its type and any input pre-processing\n    * instantiate the relevant pre-made estimator\n    * call a training, evaluation, or inference method\n  * Custom Estimator: model function, which is a method that builds graph for training, evaluation and prediction\n* Importing data: tf.data, deal with large amounts of data, different data formats and complicated transformations\n  * tf.data.Dataset: represents a sequence of elements in which each element contain one or more Tensor object\n    * create a source (Dataset.from\\_tensor\\_slices()): create a dataset from one or more tf.Tensor objects\n    * apply a transformation (Dataset.batch()): contsruct a data set from one or more tf.fdata.Dataset objects\n  * tf.data.Iterator: exxtract elements from a dataset,.\n    * Iterator.get\\_next(): the next element of a Dataset when executed and act as the interface between input pipeline code and model\n* Basic Mechanics:\n  * define a source to start an input pipeline\n  * transform the object into a new Dataset.\n> Dataset.map() —\\> apply a function to each element\n> Dataset.batch() --\\> multi-element transformation\n  * consume values baby make an iterator object that provides access to one element of the dataset at a time\n> Iterator.initializer: (re)initialize the iterator’s state\n> Iterator.get\\_next(): return tf.Tensor object that correspond to the symbolic next element\n* Dataset structure: aneleent contains one or more tf.Tensor object, each object has tf.DType and a tf.TensorShape\n  * Dataset.output\\_types, Dataset.output\\_shapes allow to insect the inferred types and shapes of each component of the a dataset element\n* Create an iterator:\n  * One-shot: support iterating once through a dataset, with no need for explicit initiilzation, do not support parameterisation\n  * initializable: run an explicit iterator, initializer operation before using it, enableparameterization of the definition of the dataset\n>  max\\_value = tf.placeholder(tf.int64, shape=[])\n> dataset=tf.data.Dataset.range(max\\_value)\n> iterator = dataset.make\\_initializable\\_iterator()\n> next\\_element=iterator.get\\_next()\n> less.run(iterator.initializer, feed\\_dict={ma\\_value:10})\n> for i in range(10):\n>  value=sess.run(next\\_element)\n>  assert i==value \n  * reinitializable: initialised from multiple different Dataset objects\n  * Feedable: use with tf.plcaeholder to select what Iterator house in each call to tf.Session.run via the feed\\_dict, none to initialise the iterator from the start of a dataset \n* Consuming values from an iterator:\n  * Iterator.get\\_next(): do not immediately advance the iterator,. use the returned tf.Tensor objects in a TensorFlow expression and pass the result of that expression to tf.Session.run() to get next elements and advance the iterator\n  * If the iterator reaches the end of the dataset, executing the Iterator,get\\_next() operation will raise a tf.errors.OutOfRangeError. Need to reinitialise the iterator\n  * A typical consumer of an iterator will include all components in a single expression\n* Reading input data:\n  * Consuming NUmPy arrays: when all the input data fit in memory,\n    * create a Dataset by converting them to tf.Tensor objects\n    * define the Dataset in terms of tf.placeholder() tensors and feed the NumPy arrays when you initialize an Iterator over the dataset\n  * Consuming TFRecord data:\n    * a simple record-oriented binary format that many TensorFlow applications use for training data\n  * Consuming text data:\n    * tf.data.TextLineDataset: extract lines from one or more text files and produce one string-valued element per line of those files\n    * lines can be removed using the Dataset.skip(). Dataset.filter() transformations\n    * Dataset.flat\\_map(): when applying these transformations to each file separately and create a nested Dataset for each file"
    }
  ]
}