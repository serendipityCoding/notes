{
  "title": "Lecture 8 Data center networking - Qos",
  "cells": [
    {
      "type": "markdown",
      "data": "1. Queueing disciplines\n  * each router must implement some queueing disciline including scheduling and drop policy\n  * Queueing allocates both bandwidth and buffer space\n  * Queueing affects latency\n  * Methods:\n    1. FIFO queueing: drop packets at the tail when buffer is full\n      * bandwidth sharing with FIFO queueing cannot be determined when multiple flows send pakets over the same link --> depend on behavior of all flows (aggresive user  can crowd out the other users\n    2. Round-Robin queueing: improve fairness of sharing bandwidth\n      * different packet size lead to bandwidth imbalance\n    3. Fair queueing: round-robin but approximate bit-level fairness by coputing virtual finish time\n      * Virtual clock ticks once for each byte sent from all flows\n      * send packets in order of their virtual finish time\n    4. Weighted Fair Queuein: assign a weight to each flow--> higher weight gives more bandwidth proportionally\n      * prooritize and protect flows \n2. Traffic shaping\n  * shape traffic flows contrains the load they out on the network\n    * limit the total traffic enables bandwidth guarantees\n    * limit burst avoids unnecessart delay and loss\n  * average rate + burstiness --> long-term + short-term\n  * Token Bucket (R, B) --> (average rate, bursts(over R) of B bits)\n    1. Tokens enter bucket at rate r and the bucket has depth b (capacity of the bucket)\n    2. If the bucket is full, the token will be discarded\n    3. Usually there are some tokens waiting to be sent, if there is a packet coming, the packet needs a token to enter the network. \n    4. So one packet goes in grab a token and enter the network\n    5. If arrival rate is smaller than r, everything goes fine\n    6. If the arrival rate exceeds the r, there will be less tokens --> not enough tokens --> wait for the tokens to accumulate --> the outgoing traffic is constraine the rate r\n    7. If there is full buckets, when there is a burst coming\n      * if the burst is lower than the rate, everything goes fine\n      * If the burst exceeds the rate, the exceeding ones are waiting for the bucket to accumulate, the outgoing traffic is still the rate r --> not large burst but small burst with stable rate\n  * Let users condition their traffic to meet the network requirements\n3. Rate delay guarantees: guarantee the minimum rate and maximum delay regardless of how other flows (interactive apps and stock trading)\n  * Admission control: decide whether to admit or reject it from the network admission control\n  * Single router rate guarantee: weighted fair queueing can guarantee rate ar a router\n  * Multiple router rate guarantee: bottleneck principle\n4. Delay guarantee: depend on the traffic flow\n  * If the flow coming in is smaller than r, the queueing delay is zero\n  * If the flow coming in exceeds r, the token bucket will reshape the traffic, the long -term traffic is going to be at most r -> there is no queueing delay\n  * If there is a large burst of packets coming in, the bucket is competely full, there will be maximum b packets coming in to the switch --> there will be some queueing delay (equals to B/R) --> because the packets coming in next to the burst is goigng to experience the queueing delay (which is the worst the queueing delay)\n  * Network delay: each router add delay\n    * even the flow pays for a burst somewhere, the output pacekts are smoothed at rate r for the next hop --> pay for the burst only once\n    * N routers delay = ntetwork delay + B/R"
    }
  ]
}